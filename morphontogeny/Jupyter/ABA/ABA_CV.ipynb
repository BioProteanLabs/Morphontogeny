{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV for NMF\n",
    "https://towardsdatascience.com/how-to-use-cross-validation-for-matrix-completion-2b14103d2c4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_matrices(X, fold):\n",
    "    \"\"\"\n",
    "    Given a matrix X, the function creates 4 sets of train + test matrices\n",
    "    where each train matrix is masked with zeros in 0.25 of the values, and the\n",
    "    test matrix is masked zeros in 0.75 of them.\n",
    "    X - numpy array\n",
    "    fold - is an integer from 0-3.\n",
    "    Returns the masked data and also the masks for train and test\n",
    "    \"\"\"\n",
    "    # Create a dict with the slicing indices\n",
    "    rows = X.shape[0]\n",
    "    cols = X.shape[1]\n",
    "    mid_rows = int(rows/2)\n",
    "    mid_cols = int(cols/2)\n",
    "    \n",
    "    idx_dict = {\n",
    "                0: [[0,mid_rows],[0, mid_cols]],\n",
    "                1: [[0,mid_rows],[mid_cols, cols]],\n",
    "                2: [[mid_rows, rows], [0, mid_cols]],\n",
    "                3: [[mid_rows, rows], [mid_cols, cols]]\n",
    "    }\n",
    "    \n",
    "    idexes = idx_dict[fold]\n",
    "    # Create masks\n",
    "    train_mask = np.full((rows, cols), 1)\n",
    "    train_mask[idexes[0][0]:idexes[0][1], idexes[1][0]:idexes[1][1]] = 0\n",
    "    test_mask = 1 - train_mask\n",
    "    \n",
    "    \n",
    "    # Create X_train\n",
    "    X_train = X.copy()\n",
    "    X_train[train_mask==0] = 0\n",
    "    \n",
    "    # Create X_test\n",
    "    X_test = X.copy()\n",
    "    X_test[train_mask==1] = 0\n",
    "        \n",
    "    return X_train, X_test, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_cv(X, latent_features, cycles=5, max_iter=25000, curRes_test_prev_init=99999):\n",
    "    \"\"\"\n",
    "    Decompose X to A*Y\n",
    "    \"\"\"\n",
    "    eps = 1e-12\n",
    "    # Main mask: take only the non-nan location for loss calculation. Later on (per fold) we combine\n",
    "    # the fold mask into one mask\n",
    "    mask = ~np.isnan(X)\n",
    "    # Fill in nan to 0\n",
    "    X = np.nan_to_num(X)\n",
    "    \n",
    "    # fold_tups is where we collect the attributes from each fold\n",
    "    fold_tups = []\n",
    "    # Cross validation by 4 folds\n",
    "    for f in range(4):\n",
    "        X_train, X_test, train_mask, test_mask = cv_matrices(X, f)\n",
    "        \n",
    "        # Creates train and test masks with nulls (combined mask) - after that we have zeros in the fold's\n",
    "        # Quartile (test area) and also in the original nulls in the train area \n",
    "        train_null_mask = mask * train_mask\n",
    "        test_null_mask = mask * test_mask\n",
    "        \n",
    "        # Takes several cycles per fold to get the minimum loss between them, due to the random initiation\n",
    "        result_list = []\n",
    "        for j in range(cycles): \n",
    "            # initial matrices. A is random and Y is A\\X.\n",
    "            rows, columns = X_train.shape\n",
    "            # A is the user matrix. Now we initiate it randomly\n",
    "            A = np.sum(np.nan_to_num(np.diag(X_train)))*np.random.randn(rows, latent_features)/np.sqrt(latent_features) \n",
    "            A = np.maximum(A, eps)\n",
    "\n",
    "            # Y is the product matrix. We initiate it is a way that given A, we get the minimum Frobenius Form\n",
    "            Y = linalg.lstsq(A, X_train)[0]\n",
    "            Y = np.maximum(Y, eps)\n",
    "            \n",
    "            # Get only the train values\n",
    "            masked_X = train_null_mask * X_train\n",
    "            \n",
    "            # Set test residual at a high value and train at 0\n",
    "            curRes_test_prev = curRes_test_prev_init\n",
    "            curRes_test_best = curRes_test_prev_init\n",
    "            curRes_train_prev = 0\n",
    "  \n",
    "            train_list = [] # per cycle\n",
    "            test_list = [] # per cycle\n",
    "            \n",
    "            # Initialize the resulted X (per cycle)\n",
    "            X_est_prev = None\n",
    "            X_est = np.dot(A, Y)\n",
    "            \n",
    "            # Takes several cycles per fold to get the minimum loss between them, due to the random initiation\n",
    "        result_list = []\n",
    "        for j in range(cycles): \n",
    "            # initial matrices. A is random and Y is A\\X.\n",
    "            rows, columns = X_train.shape\n",
    "            # A is the user matrix. Now we initiate it randomly\n",
    "            A = np.sum(np.nan_to_num(np.diag(X_train)))*np.random.randn(rows, latent_features)/np.sqrt(latent_features) \n",
    "            A = np.maximum(A, eps)\n",
    "\n",
    "            # Y is the product matrix. We initiate it is a way that given A, we get the minimum Frobenius Form\n",
    "            Y = linalg.lstsq(A, X_train)[0]\n",
    "            Y = np.maximum(Y, eps)\n",
    "            \n",
    "            # Get only the train values\n",
    "            masked_X = train_null_mask * X_train\n",
    "            \n",
    "            # Set test residual at a high value and train at 0\n",
    "            curRes_test_prev = curRes_test_prev_init\n",
    "            curRes_test_best = curRes_test_prev_init\n",
    "            curRes_train_prev = 0\n",
    "  \n",
    "            train_list = [] # per cycle\n",
    "            test_list = [] # per cycle\n",
    "            \n",
    "            # Initialize the resulted X (per cycle)\n",
    "            X_est_prev = None\n",
    "            X_est = np.dot(A, Y)\n",
    "            \n",
    "            # This for loop is the main algorithm procedure, updates the matrix till minimum residual or till break\n",
    "            i = -1\n",
    "            while X_est_prev is None or fit_residual > 1e-9:\n",
    "                i += 1\n",
    "                X_est_prev = X_est\n",
    "                # Update A\n",
    "                top = np.dot(masked_X, Y.T)\n",
    "                bottom = (np.dot((train_null_mask * np.dot(A, Y)), Y.T)) + eps\n",
    "                A *= top / bottom\n",
    "                A = np.maximum(A, eps)\n",
    "                # Update Y\n",
    "                top = np.dot(A.T, masked_X)\n",
    "                bottom = np.dot(A.T, train_null_mask * np.dot(A, Y)) + eps\n",
    "                Y *= top / bottom\n",
    "                Y = np.maximum(Y, eps)\n",
    "                # New X matrix based on updated A and Y\n",
    "                X_est = np.dot(A, Y)\n",
    "                                     \n",
    "                err = train_null_mask * (X_est_prev - X_est)\n",
    "                fit_residual = np.sqrt(np.sum(err ** 2))\n",
    "                if i > max_iter:\n",
    "                    break\n",
    "                    \n",
    "                    # Collect results per cycle\n",
    "            result_list.append((A, Y, curRes_train_prev, curRes_test_prev, i, train_list, test_list))\n",
    "\n",
    "        result_list_sorted = list(sorted(result_list, key=lambda x: x[2]))\n",
    "        # Get all the attributes of the best cycle per fold\n",
    "        fold_best_curRes_test = result_list_sorted[0][3]\n",
    "        fold_best_curRes_train = result_list_sorted[0][2]\n",
    "        fold_A = result_list_sorted[0][0]\n",
    "        fold_Y = result_list_sorted[0][1]\n",
    "        fold_tup = (fold_A, fold_Y, fold_best_curRes_train, fold_best_curRes_test)\n",
    "        fold_tups.append(fold_tup)\n",
    "        \n",
    "        \n",
    "    # Get avg train/test score from all folds\n",
    "    train_mean = np.mean([x[2] for x in fold_tups])\n",
    "    test_mean = np.mean([x[3] for x in fold_tups])\n",
    "    # Get all combinaitions of A and Y\n",
    "    mean_A = [x[0] for x in fold_tups]\n",
    "    mean_Y = [x[1] for x in fold_tups]\n",
    "    \n",
    "    return train_mean, test_mean, mean_A, mean_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple k and capture results\n",
    "\n",
    "n_iter = 25000\n",
    "k_tup_list = []\n",
    "# k must be lower than the min of (columns/rows)\n",
    "# k_max = min(orig_matrix_round_plus.shape[0], orig_matrix_round_plus.shape[1])\n",
    "k_max = 400\n",
    "X = np.memmap('/data/bioprotean/ABA/MEMMAP/genes_list/finalgenes_pos_L2.mymemmap',\\\n",
    "dtype='float32', mode='r', shape=(159326,2941))\n",
    "\n",
    "for k in range(2,k_max):\n",
    "    \n",
    "    train_mean, test_mean, final_As, final_Ys = nmf_cv(X, k, max_iter=n_iter)\n",
    "    k_tup = (train_mean, test_mean, final_As, final_Ys, k)\n",
    "    k_tup_list.append(k_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the best run\n",
    "# Get the min test error and its index\n",
    "test_err = [tup[1] for tup in k_tup_list]\n",
    "min_test_err = np.min(test_err)\n",
    "min_test_err_idx = test_err.index(min_test_err)\n",
    "# Extract the run and its attributes\n",
    "best_run = k_tup_list[min_test_err_idx]\n",
    "best_k = min_test_err_idx + 2 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
